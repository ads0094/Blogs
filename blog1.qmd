---
format: html
editor: visual
html:
  includes:
    in-header: blog-1_styles.css
---

<link rel="stylesheet" href="blog-1_styles.css">

### Inferential Statistics: Statistics & Parameters

#### Population:

A population is a “group” that a researcher is interested in studying, for example if the researcher wants to find the height of all the adult US male population then ALL the male adults in the US form the population for our researcher. A researcher can choose the **common traits** that form the population for their study. For example a researcher might be interested in studying the height of adult US male population with the red hair. Researcher can continue to add the traits that define his population. However, adding too many traits may limit the eventual size of the population which also limits the groups to which the findings can be rightfully extrapolated. Furthermore, the population may be finite or infinite, and if all the members of the population are included then its called census and the values obtained are called the census data.

#### Parameter

A parameter is any measure obtained by studying the population. Given that measuring the entire population is often times impossible, population parameters (denoted by $\theta$) are often estimated or inferred.

#### Sample

A sample subset of the population that is selected for study instead. Larger the population the better the findings from the sample will be, although they need not to be too big if the chosen sample is representative of the population.

#### Statistic

Just as the parameter is measured from the population, statistic is measured from the sample. For example if the researcher wants to find out the average (mean) height of male students in the Engineering department at the given university, and given there are thousands of students enrolled in the engineering department, the researcher decides to select the subset of the population —a sample for his study. The **average (mean) height** of the students derived from his sample is called the **sample statistic**, often denoted with “bar”, like $\bar x$, whereas the average height of all the male students in the engineering department at the given university is called the **population parameter**., often denoted with $\theta$.

> Inferential statistics is the technique of predicting unknown parameters on the basis of the known statistics — the act of generalizing to the many after having observed the few.

#### Techniques of the sampling:

To make accurate predictions, a sample must be **representative** of the population. This means the sample should mirror the population in terms of **key characteristics**.

A representative sample provides a "miniature mirror" of the population, allowing researchers to generalize findings accurately.

Two key techniques for obtaining representative samples are **random sampling** and **stratified or quota sampling**.

#### Random Sampling

**Definition**: Random sampling is a method where every member of the population has an **equal** **chance** of being selected, ensuring that no subgroups are systematically excluded. True random sampling is deliberate and structured, not haphazard, to avoid bias.

**Challenges in Achieving True Random Sampling:** For instance, selecting people based on their availability at specific times of the day excludes those who are not available, leading to a biased sample. The goal is to ensure that every individual in the population has an equal chance of being selected, without any systematic exclusions.

**Self-Selected Samples and Bias: I**f individuals select themselves into the sample (e.g., by choosing to respond to a survey), the sample is not random and will likely be biased. Examples include surveys where participants choose to respond, which attracts a certain type of respondent (those with strong opinions or more time), leading to non-representative results.

**The Importance of Non-Biased Sampling in Research: Non-biased**, random sampling is crucial for accurate research and generalization. It warns against the common pitfalls of non-random sampling, such as allowing individuals to select themselves into a study or using convenience sampling methods that systematically exclude certain groups.

In summary, the text explains the critical importance of random and representative sampling in research, the challenges in achieving true random sampling, and the biases introduced by self-selected samples. The emphasis is on the need for careful and deliberate sampling methods to ensure that the sample accurately reflects the population and allows for valid generalizations.

#### Stratified or Quota Sampling

Another technique for selecting a representative sample is stratified sampling. This is sometimes combined with the random sampling in such a way that first the researcher will identify the subgroups within the population based on certain characteristics and then take random samples from each subgroup to make the overall sample representative of the population. For example if the researcher interested in studying the education levels of adults in city. The researcher can identify many characteristics that differentiate different groups of people such as neighborhoods, race, gender, and many more. The researcher will then find out the proportions of population with different characteristics through government census data, and then generate random samples from the each subgroup representative of their actual proportion in the population. For example if 32% of the population are non-white people then researcher will generate random samples of non-white population that will make up 32% of his total sample size. For example if the total sample size is 1000 then 320 of these will be non-white people randomly chosen.

### Sampling Error

Sampling error is simply the difference between the population parameter and the sample statistics for example if the population mean ($\mu$) is 39 and the sample mean $\bar x$ is 37 then the sampling error is $39-37=2$. The sampling error is natural deviation between the population parameter and our estimated statistic. This sampling error is random meaning that it can go either way; it can either be positive or negative. The probability of the sample mean being less than the population mean is 0.5 because the probability that a sample mean will be above the population mean is 0.5. The sampling error under the normal should cancel out each other meaning that 50% of the time there should be negative errors and 50% of the times there should be positive errors.

### Outliers

These are the extreme values usually 3 or more SDs away from the mean. Outliers can indicate that the distribution is not normal or that some error is has occurred.

### Bias

Bias occurs when the sample differs from the population systematically at large. Simply put, Bias is when all the sampling errors load up on one side which severely underestimate or overestimate the population parameter (mean, usually). This can be due to sampling bias, selection bias any other form bias makes the sample less representative of the actual population and thus, the findings from this sample are a “bias”.

## Sampling distribution

In the sampling distributions, each point on the x-axis represents the groups/sample performance (usually arithmetic mean).

### Mean of the distribution of Means

Sampling distribution is when we take many samples from the population, compute the mean of the each of the samples and then create a distribution of the sample means obtained from all the samples by adding up all the sample means and dividing by the number of samples. This is different from the individual points within the sample. When there is only one sample, then the researcher would compute the mean and standard deviation to see how spread out the sample data is from its mean. Sampling distribution requires the researcher to compute the mean for many samples taken from the population and then create a distribution of the sample means. This allows the researcher to see how spread out the sample means are from each other, and the mean of sampling distribution (mean of the means of the distribution). This can look like this: $M_1=x_1, M_2=x_2, M_3=x_3, ... M_n=x_n$. The sample distribution mean can be $\bar X$. This mean of the distribution of the sample means is equal to the parameter mean \$\\mu\$. Therefore, $M_M=\mu$ where $M_M$ is the mean of the distribution of the sample means — a kind of super-mea

#### Standard Error of the Mean

**Standard deviation Vs the Standard error of the mean**

The SD of the sample is denoted by SD whereas the standard deviation of the population is denoted by the $\sigma$.

$\sigma$ is used to measure the variability within the entire population.

Standard deviation $(s)$ measures the variability within a sample. However, this variability is an estimate because the sample is only a part of the population.

Comparing the Two Standard Deviations: Distribution of Individual Scores vs. Distribution of Means, we know that the standard deviation of the sample means is also known as the standard error of the mean.

The standard deviation of the distribution of means ( $\sigma_{\bar{X}}$ ) or the standard error of the mean when the population $\sigma$ is known, is given by: $\sigma_{\bar{X}} = \frac{\sigma}{\sqrt{n}}$

Where:

-   $\sigma$ is the standard deviation of the population.

-    $n$ is the sample size.

This implies: a**s the sample size increases, the standard deviation of the distribution of means or SEM decreases**. This means that larger samples lead to more precise estimates of the population mean.

As an example if 30 students are measured and if you were to average the scores of many groups of 30 students, the average scores (means) would vary less than the individual scores within each group. This reduced variability is captured by the standard error.

The **Standard Error of the Mean (SEM)** is a statistical measure that **estimates the variability of the sample mean relative to the true population mean**. It essentially tells you how much the sample means $\bar X$ is expected to fluctuate from the actual population mean $\mu$ if you were to take multiple samples from the same population.

The SEM is the standard deviation of the sampling distribution of the sample mean. It quantifies the precision of the sample mean as an estimate of the population mean.

**Formula**: $\text{SEM} = \frac{\sigma}{\sqrt{n}}$

-   $\sigma$: Population standard deviation.

-   $n$: Sample size.

If the **population standard deviation** $\sigma$ **is unknown**, the sample standard deviation $s$ can be used as an estimate: $\text{SEM} = \frac{s}{\sqrt{n}}$

In this case we denote the SEM as $s_{\bar X}$.

**Interpretation**:

-   A **smaller SEM** indicates that the sample mean is a more accurate reflection of the population mean, meaning there is less variability in the sample means across different samples.

-   A **larger SEM** suggests more variability among sample means, implying that the sample mean might not be as close to the population mean.

**Application**:

-   **Confidence Intervals**: SEM is often used to construct confidence intervals for the population mean. For instance, a 95% confidence interval gives a range that likely contains the true population mean.

-   **Hypothesis Testing**: In hypothesis testing, SEM is used to assess whether a sample mean significantly differs from a hypothesized population mean.

**Example 1: ONE sample from the Population:**

(In this example we do **not** have a sampling distribution)

Suppose you have a sample of 100 students, and you want to estimate the average height of all students at a university. You calculate the sample mean height $\bar{x}$ and the sample standard deviation $s$.

-   **Sample Size (n)**: 100

-   **Sample Standard Deviation (s)**: 5 cm

The **Standard Error of the Mean** is calculated as: $\text{SEM} = \frac{s}{\sqrt{n}} = \frac{5}{\sqrt{100}} = \frac{5}{10} = 0.5 \text{ cm}$

Note that here we have used one large sample n=100 to compute the mean and standard deviation. And then used this standard deviation to compute SEM. The advantage is that the large sample size takes effect on the precision of our estimates making it more reliable and giving us a lower SEM.

**Summary:**

The **Standard Error of the Mean (SEM)** provides a measure of how well a sample mean estimates the true population mean. A lower SEM indicates a more reliable sample mean, while a higher SEM suggests more variability and less reliability. It plays a crucial role in statistical inference, particularly in constructing confidence intervals and conducting hypothesis tests.

We can compute the SEM for multiple samples.

**Example 2: Multiple samples from the Population**

In this example we do have a sampling distribution as we are taking multiple samples from the population.

**Step 1: Define a Population**

We don't know the exact population mean or standard deviation, and our goal is to estimate them using samples.

-   **Population**: Test scores of all students in a large school.

-   **Population Mean**: Unknown.

-   **Population Standard Deviation**: Unknown.

**Step 2: Take Multiple Samples from the Population**

We take 4 random samples, each consisting of 5 students’ test scores.

-   **Sample 1**: 78, 82, 85, 90, 92

-   **Sample 2**: 72, 88, 80, 76, 84

-   **Sample 3**: 95, 92, 85, 87, 90

-   **Sample 4**: 68, 75, 70, 80, 82

**Step 3: Calculate the Mean for Each Sample**

Now, we calculate the mean of each sample:

1.   Mean of Sample 1: $\bar{X}_1 = \frac{78 + 82 + 85 + 90 + 92}{5} = \frac{427}{5} = 85.4$

2.   Mean of Sample 2: $\bar{X}_2 = \frac{72 + 88 + 80 + 76 + 84}{5} = \frac{400}{5} = 80$

3.   Mean of Sample 3: $\bar{X}_3 = \frac{95 + 92 + 85 + 87 + 90}{5} = \frac{449}{5} = 89.8$

4.   Mean of Sample 4: $\bar{X}_4 = \frac{68 + 75 + 70 + 80 + 82}{5} = \frac{375}{5} = 75$

**Step 4: Create a Distribution of the Sample Means**

Now that we have the means of the four samples:

-    Sample 1 Mean: 85.4

-    Sample 2 Mean: 80

-    Sample 3 Mean: 89.8

-    Sample 4 Mean: 75

We can now create a distribution of these sample means.

**Step 5: Calculate the Mean of the Sample Means**

$\text{Mean of Sample Means} = \frac{85.4 + 80 + 89.8 + 75}{4} = \frac{330.2}{4} = 82.55$ ⇒ we can call this the **grand mean** $(\bar {\bar {x}})$.

So, the mean of the sample means is $\bar {\bar {x}}$= **82.55**.

**Step 6: Calculate the Standard Error of the Mean (SEM)**

**Step 6.1: Calculate the Standard Deviation of the Sample Means**

To calculate the SEM, we need the standard deviation of the sample means. We will treat the sample means as data points and calculate their standard deviation.

1.   **Deviations of Each Sample Mean from the Mean of Sample Means**:

    -   $(85.4 - 82.55)^2 = 8.1225$

    -   $(80 - 82.55)^2 = 6.5025$

    -   $(89.8 - 82.55)^2 = 52.0225$

    -   $(75 - 82.55)^2 = 57.0025$

2.   **Sum the Squared Deviations**: $\text{Sum of Squared Deviations} = 8.1225 + 6.5025 + 52.0225 + 57.0025 = 123.65$

3.   **Calculate the Variance** by dividing by $n-1$ (degrees of freedom, where $n = 4$: $\text{Variance} = \frac{123.65}{3} \approx 41.2167$

4.   **Calculate the Standard Deviation** $s$ by taking the square root of the variance: $s = \sqrt{41.2167} \approx 6.42$: this $s$ is the standard deviation of the sample means: variability around the grand mean $(\bar {\bar {x}})$

**Step 6.2: Use the Standard Deviation to Calculate SEM**

Now that we have the standard deviation of the sample means (\$s = 6.42\$), we can calculate the **SEM** using the formula: $\text{SEM} = \frac{s}{\sqrt{n}} = \frac{6.42}{\sqrt{4}} = \frac{6.42}{2} = 3.21$

Final Answer:

-   **Mean of the Sample Means**: 82.55

-   **Standard Error of the Mean (SEM)**: 3.21

> **Central Limit Theorem**: In short this theory states that if we take a large enough sample (usually \>30) then our sampling distribution approaches normality.

#### Z-Scores in the Context of Sampling Distribution

After having calculated the SEM from multiple samples, the next step is often to construct the confidence interval — an interval that contains the true parameter value with a certain degree of confidence such as 80%, 90%, 95% (most common) and 99%. Our hypothesised $\mu$, which we have estimated using a large sample \$(n\\ge 30)\$, is our point estimate. As mentioned this is an estimate of the population mean $\mu$, and due to sampling error this estimate of the mean may not be valid, but it is our best guess and likely very close to the true population mean.

For a z-test, you need to know:

1.   The population standard deviation (σ)

2.   Either: a) The population mean (μ) if you're comparing a sample to a known population OR b) The hypothesised difference between means if you're comparing two populations

It's important to note:

-   You don't always need to know the population mean for all types of z-tests.

-   The critical element is knowing the population standard deviation (σ).

Here are the common scenarios:

1.   One-sample z-test:

    a\. You know the population standard deviation (σ)

    b\. You know or hypothesize a population mean (μ)

    c\. You compare your sample mean to this known/hypothesized population mean

2.   Two-sample z-test:

    a\. You know the population standard deviations for both populations

    b\. You don't necessarily need to know the population means

    c\. You're testing if there's a significant difference between the two population means

3.   Z-test for proportions:

    a\. Used for categorical data

    b\. You don't need to know means or standard deviations

    c\. You use the properties of the binomial distribution

So, to summarize: for a z-test, you always need to know the population standard deviation(s), but you don't always need to know the population mean(s). The population mean might be what you're testing against or trying to estimate.

In the context of a **sampling distribution**, z-scores are used to determine how unusual or typical a sample mean is relative to the population mean. The **z-test** is a statistical method that allows you to test whether the difference between a sample mean and the known population mean is statistically significant or whether it could be attributed to sampling error. OR if we are computing the means for to population, we need to know the hypothesised difference between the two populations.

Example:

-   **Population Mean (μ)**: 100

-   **Population Standard Deviation (σ)**: 15

-   **Sample Mean (** $\bar{x}$ **)**: 95

-   **Sample Size (n)**: 36

To determine if the difference between the sample mean and the population mean is due to random sampling error or if the sample is likely not from the same population we conduct a **Z test.**

### Steps to Conduct a Z-Test:

1.  **Calculate the Standard Error of the Mean (SEM):**

2.   The SEM represents the standard deviation of the sampling distribution of the sample mean. It is calculated using the formula: $\text{SEM} = \frac{\sigma}{\sqrt{n}} = \frac{15}{\sqrt{36}} = \frac{15}{6} = 2.5$

3.  **Calculate the Z-Score:**

The z-score tells you how many standard errors the sample mean is away from the population mean. The formula for calculating the z-score in this context is: $z = \frac{\bar{x} - \mu}{\text{SEM}} = \frac{95 - 100}{2.5} = \frac{-5}{2.5} = -2$ This z-score of -2 means that the sample mean is 2 standard errors below the population mean.

3.  **Compare the Z-Score to the Critical Value:**

In many cases, you use a z-table or a standard normal distribution table to find the probability associated with the z-score. However, in hypothesis testing, you often compare the z-score to a critical value. The critical value depends on the confidence level you are working with:

-   **For a 95% confidence level**, the critical z-values are **±1.96**.

Since our calculated z-score is -2, which is less than -1.96, it falls outside the range of what is typically expected if the sample were drawn from this population.

**Interpretation:**

-   **If the z-score falls within the range of -1.96 to 1.96**, the difference between the sample mean and the population mean could likely be due to random sampling error.

-   **If the z-score falls outside this range**, as it does in our example, it suggests that the sample mean is significantly different from the population mean. Therefore, it’s unlikely that this sample comes from the population with a mean of 100.

**Understanding the 1.96 Rule:**

The "1.96 rule" corresponds to a **95% confidence interval** in a standard normal distribution. It means that if the sample mean is more than 1.96 standard errors away from the population mean, there is less than a 5% probability that the sample mean is from the same population (under the assumption of a normal distribution).

### Conclusion:

In this example, because the z-score of -2 falls outside the critical range of ±1.96, we would reject the null hypothesis that the sample mean is equal to the population mean. This suggests that the observed difference is statistically significant and not just due to random sampling error. It raises the possibility that this sample may come from a different population.
